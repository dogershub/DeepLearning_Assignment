{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. Describe the decision tree classifier algorithm and how it works to make predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Decision Tree Classifier algorithm is a simple and intuitive machine learning method used for classification tasks, such as determining whether an input belongs to one of several predefined categories. Here's a simplified explanation of how it works:\n",
    "\n",
    "1. **Building the Tree:** The algorithm starts with a dataset that contains labeled examples, where each example has several features (attributes) and a corresponding class label. The algorithm aims to create a tree-like structure where each node in the tree represents a decision based on one of the features.\n",
    "\n",
    "2. **Choosing the Best Feature:** At each node of the tree, the algorithm evaluates all the features and selects the one that provides the best separation of the data into different classes. It does this by measuring the \"purity\" of the data, meaning how well it separates into distinct categories based on the chosen feature.\n",
    "\n",
    "3. **Splitting the Data:** The dataset is split into subsets based on the chosen feature's values. Each branch represents a different value or range of values for the selected feature.\n",
    "\n",
    "4. **Repeat:** The algorithm repeats this process for each branch or subset, creating new nodes with new feature choices until it reaches a stopping point. This could be when the tree reaches a certain depth, a minimum number of samples, or when it can no longer improve the separation of classes.\n",
    "\n",
    "5. **Leaf Nodes:** The final nodes, called leaf nodes, represent the predicted class labels. When a new input is provided, the decision tree traverses the tree by following the feature-based decisions until it reaches a leaf node. The class label associated with that leaf node is the prediction.\n",
    "\n",
    "6. **Prediction:** To make a prediction, you start at the tree's root and follow the path determined by the input's feature values. You eventually reach a leaf node, and the class label associated with that leaf node is the predicted class for the input.\n",
    "\n",
    "In summary, the Decision Tree Classifier algorithm learns to make predictions by creating a tree structure that uses a series of feature-based decisions to classify inputs into different categories. It's a powerful and interpretable algorithm, making it a valuable tool for a wide range of classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example:\n",
    "\n",
    "We load the Iris dataset, which is a dataset for classifying iris plant species into three classes.\n",
    "We split the dataset into a training set and a testing set to train and evaluate the model.\n",
    "We create a Decision Tree Classifier, clf, and fit it to the training data using fit.\n",
    "We make predictions on the test data using predict.\n",
    "We calculate the accuracy of the model by comparing the predicted values to the true labels.\n",
    "Finally, we use the trained model to make a prediction for a new sample.\n",
    "This code demonstrates how to create, train, and use a Decision Tree Classifier for a classification problem, and how to make predictions for new samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Sepal Length | Sepal Width | Petal Length | Petal Width | Species |\n",
    "|--------------|------------|-------------|------------|---------|\n",
    "| 5.1          | 3.5        | 1.4         | 0.2        | Setosa  |\n",
    "| 4.9          | 3.0        | 1.4         | 0.2        | Setosa  |\n",
    "| 7.0          | 3.2        | 4.7         | 1.4        | Versicolor |\n",
    "| 6.4          | 3.2        | 4.5         | 1.5        | Versicolor |\n",
    "| 6.3          | 3.3        | 6.0         | 2.5        | Virginica  |\n",
    "| ...          | ...        | ...         | ...        | ...      |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.00\n",
      "Predicted class for the new sample: setosa\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset (a well-known dataset for classification)\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Decision Tree Classifier\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "# Fit (train) the classifier on the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "# Predict a new sample (example)\n",
    "new_sample = [[5.1, 3.5, 1.4, 0.2]]  # Example input features\n",
    "prediction = clf.predict(new_sample)\n",
    "print(f'Predicted class for the new sample: {data.target_names[prediction][0]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. Provide a step-by-step explanation of the mathematical intuition behind decision tree classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision tree classification is based on a series of mathematical and statistical concepts. Here's a step-by-step explanation of the mathematical intuition behind decision tree classification:\n",
    "\n",
    "1. Entropy and Information Gain:\n",
    "\n",
    "- The decision tree classification algorithm uses entropy to measure the impurity or disorder in a dataset.\n",
    "\n",
    "- Entropy is a mathematical concept borrowed from information theory.\n",
    "\n",
    "- Where p_i represents the proportion of samples in class i in the dataset.\n",
    "\n",
    "- The higher the entropy, the greater the disorder. The goal of the decision tree is to reduce entropy, making the dataset more pure by splitting it into subsets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H(S) = -Σ (p_i * log2(p_i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Information Gain:\n",
    "\n",
    "- Information gain is used to evaluate the effectiveness of splitting the dataset based on a feature. It measures how much the entropy is reduced when a dataset is split.\n",
    "\n",
    "- Information gain (IG) is calculated as the difference between the entropy before the split (H(S)) and the weighted average of entropies after the split (H(S|A)):\n",
    "\n",
    "- Where S_v represents the subset of data for a particular value of the feature A, and |S| represents the size of the dataset.\n",
    "\n",
    "- Decision tree algorithms select the feature that maximizes information gain, as it leads to the most significant reduction in entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IG(S, A) = H(S) - Σ (|S_v| / |S|) * H(S_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Splitting Criteria:\n",
    "Decision tree algorithms use various splitting criteria to determine how to divide the dataset based on feature values. Common criteria include Gini impurity and mean squared error for regression tasks.\n",
    "For classification, Gini impurity measures the probability of misclassifying a randomly chosen element if it were randomly labeled according to the distribution of the dataset.\n",
    "\n",
    "- Where p_i is the probability of an element being in class i.\n",
    "\n",
    "- The split that minimizes the Gini impurity or maximizes information gain is chosen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gini(S) = 1 - Σ (p_i)^2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Recursive Partitioning:\n",
    "Decision tree construction involves recursively partitioning the dataset. At each node, the algorithm selects the feature that best splits the data based on the chosen criterion (entropy, Gini impurity, etc.).\n",
    "The dataset is divided into subsets based on the values of the selected feature.\n",
    "This process continues recursively until stopping conditions are met (e.g., maximum depth, minimum samples per leaf, or other hyperparameters).\n",
    "\n",
    "5. Predictions:\n",
    "Once the tree is constructed, it can be used for predictions. To classify a new data point, it traverses the tree from the root to a leaf node based on feature values.\n",
    "The class label of the majority of training samples in that leaf node is assigned to the new data point.\n",
    "\n",
    "In summary, decision tree classification relies on mathematical concepts such as entropy, information gain, and impurity measures to construct a tree structure that recursively partitions the dataset. By selecting the feature that minimizes impurity or maximizes information gain, the algorithm creates an effective model for classifying new data points based on their feature values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3. Explain how a decision tree classifier can be used to solve a binary classification problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A decision tree classifier can be used to solve a binary classification problem by creating a tree-like structure that makes a sequence of decisions to classify data into one of two categories or classes (e.g., yes/no, 0/1, true/false). Here's how it works:\n",
    "\n",
    "1. **Data Collection**: Gather a dataset that includes features (independent variables) and labels (the binary classification: 0 or 1, for instance). Each data point consists of a set of features and a label.\n",
    "\n",
    "2. **Tree Building**: The decision tree classifier algorithm builds the tree structure based on the dataset. It selects the features to split the data at each node, trying to reduce impurity or increase information gain. The tree is constructed recursively.\n",
    "\n",
    "3. **Node Splitting**: At each node, the decision tree algorithm chooses a feature and a threshold that best splits the data into two subsets. The choice is made to minimize impurity or maximize information gain. The data points are divided into \"left\" and \"right\" based on whether they satisfy the splitting condition. This process continues until a stopping criterion is met (e.g., a maximum depth is reached, or there are too few samples in a node).\n",
    "\n",
    "4. **Leaf Nodes**: When the tree-building process is complete, the terminal nodes are called leaf nodes. Each leaf node represents a binary classification decision. For example, one leaf node might indicate that the instance belongs to class 1, while another indicates class 0.\n",
    "\n",
    "5. **Classification**: To classify a new data point, it is passed down the tree, starting at the root node and traversing the branches based on the values of the features. The data point moves left or right through the tree nodes based on whether the feature values satisfy the splitting conditions. It eventually reaches a leaf node, and the label associated with that leaf node is the classifier's prediction.\n",
    "\n",
    "### Example: Sentiment Analysis\n",
    "Let's consider an example of binary sentiment analysis using a decision tree classifier. The goal is to classify movie reviews as either \"positive\" or \"negative\" based on the text content.\n",
    "\n",
    "1. **Data Collection**: Gather a dataset of movie reviews with associated labels: 1 for positive reviews and 0 for negative reviews.\n",
    "\n",
    "2. **Feature Extraction**: The features might include word frequencies, sentiment words, or other relevant attributes extracted from the text.\n",
    "\n",
    "3. **Tree Building**: The decision tree classifier is trained on the dataset, making decisions based on the features. It selects the most informative features (e.g., specific words or phrases) for splitting the data.\n",
    "\n",
    "4. **Node Splitting**: The tree algorithm decides how to split data based on feature values. For example, it might decide that if the word \"amazing\" appears in a review, the sentiment is positive. If not, it might look for other features, such as the word \"terrible,\" to classify the review as negative.\n",
    "\n",
    "5. **Leaf Nodes**: The tree construction continues until leaf nodes are reached. Each leaf node represents a classification decision, such as \"positive\" or \"negative.\"\n",
    "\n",
    "6. **Classification**: To classify a new movie review, the decision tree examines the text and decides which branch to follow based on the presence or absence of specific words. It eventually reaches a leaf node and assigns a sentiment label.\n",
    "\n",
    "Binary classification problems, such as sentiment analysis, spam detection, and medical diagnosis (e.g., disease presence/absence), can be effectively solved using decision tree classifiers. They provide interpretable models and can make accurate predictions for a wide range of applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4. Discuss the geometric intuition behind decision tree classification and how it can be used to make predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The geometric intuition behind decision tree classification is closely related to the concept of hyperplanes and space partitioning in feature space. It involves finding decision boundaries in the feature space that separate data points belonging to different classes. Let's break down the geometric intuition and the math involved:\n",
    "\n",
    "1. **Feature Space**: Consider a binary classification problem with two features, feature 1 (x-axis) and feature 2 (y-axis). The data points are plotted in a two-dimensional feature space.\n",
    "\n",
    "2. **Binary Decision**: A decision tree classifier starts with a root node, which represents the entire feature space. Each internal node of the tree corresponds to a binary decision on one of the features. For example, it might ask whether \"Feature 1\" is greater than a certain threshold. If the answer is yes, the data point moves to the right; if no, it moves to the left.\n",
    "\n",
    "3. **Hyperplanes**: The binary decisions correspond to hyperplanes in the feature space. In a two-dimensional space, a decision boundary is a straight line (a hyperplane) that divides the space into two regions.\n",
    "\n",
    "4. **Recursive Splitting**: Decision trees are constructed recursively. At each internal node, a binary decision is made to split the data into two subsets. The tree keeps branching until a stopping criterion is met. Stopping criteria could include a maximum depth, a minimum number of data points in a node, or a lack of improvement in impurity measures.\n",
    "\n",
    "5. **Classification Regions**: The decision tree partitions the feature space into regions, each associated with a class label. For a binary classification, there are two classes, and the regions correspond to the \"positive\" and \"negative\" classes.\n",
    "\n",
    "6. **Predictions**: To make a prediction for a new data point, you start at the root node of the tree and follow the binary decisions based on the feature values. The path through the tree leads to a leaf node, which represents the predicted class label.\n",
    "\n",
    "Mathematically, you can express the decision boundaries using equations. Suppose you have a feature vector x = [x₁, x₂], and you reach an internal node in the decision tree, which corresponds to a binary decision. For example, the decision might be x₁ > θ, where θ is a threshold.\n",
    "\n",
    "- If x₁ > θ, you move to the right child node.\n",
    "- If x₁ ≤ θ, you move to the left child node.\n",
    "\n",
    "The decision boundaries are described by equations of the form:\n",
    "\n",
    "- Right child region: x₁ > θ\n",
    "- Left child region: x₁ ≤ θ\n",
    "\n",
    "These equations represent the hyperplanes that split the feature space into different regions.\n",
    "\n",
    "To visualize these decision boundaries, you can plot the tree structure with the regions it defines, which shows how the tree classifies data points based on their feature values.\n",
    "\n",
    "In summary, the geometric intuition behind decision tree classification involves partitioning the feature space into regions defined by decision boundaries (hyperplanes) created through binary decisions. The math is expressed as threshold-based conditions on feature values, guiding data points to their predicted classes based on the decision tree's structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5. Define the confusion matrix and describe how it can be used to evaluate the performance of a classification model.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A confusion matrix is a tabular representation used in the field of machine learning and classification to evaluate the performance of a classification model. It summarizes the results of a classification problem by presenting the actual and predicted class labels for a dataset. The confusion matrix is particularly useful when dealing with binary classification problems, but it can be extended to multi-class problems as well. It provides valuable insights into the model's performance, including the ability to distinguish between different types of errors and assess its overall accuracy.\n",
    "\n",
    "A standard confusion matrix is structured as follows:\n",
    "\n",
    "True Positive (TP): The model correctly predicted the positive class (e.g., class 1).\n",
    "True Negative (TN): The model correctly predicted the negative class (e.g., class 0).\n",
    "False Positive (FP): The model incorrectly predicted the positive class (false alarm or Type I error).\n",
    "False Negative (FN): The model incorrectly predicted the negative class (miss or Type II error).\n",
    "Here's a graphical representation of a binary classification confusion matrix:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|                   | Actual Positive (1) | Actual Negative (0) |\n",
    "|-------------------|----------------------|-----------------------|\n",
    "| Predicted Positive| True Positive (TP)  | False Positive (FP)  |\n",
    "| (Negative)        |                      |                      |\n",
    "|-------------------|----------------------|-----------------------|\n",
    "| Predicted Negative| False Negative (FN)  | True Negative (TN)  |\n",
    "| (Negative)        |                      |                      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The elements of the confusion matrix allow us to compute various performance metrics for the classification model, such as:\n",
    "\n",
    "1. Accuracy: Accuracy measures the proportion of correctly predicted instances out of the total number of instances. It's calculated as (TP + TN) / (TP + TN + FP + FN).\n",
    "\n",
    "2. Precision (Positive Predictive Value): Precision focuses on the accuracy of positive predictions. It's calculated as TP / (TP + FP) and is useful when minimizing false positives is crucial.\n",
    "\n",
    "3. Recall (Sensitivity or True Positive Rate): Recall measures the ability of the model to identify all relevant instances (true positives). It's calculated as TP / (TP + FN) and is important when minimizing false negatives is a priority.\n",
    "\n",
    "4. Specificity (True Negative Rate): Specificity measures the ability of the model to identify all non-relevant instances (true negatives). It's calculated as TN / (TN + FP).\n",
    "\n",
    "5. F1 Score: The F1 score is the harmonic mean of precision and recall, providing a balance between these two metrics. It's calculated as 2 * (Precision * Recall) / (Precision + Recall).\n",
    "\n",
    "6. ROC Curve and AUC: The Receiver Operating Characteristic (ROC) curve shows the trade-off between sensitivity (true positive rate) and specificity (true negative rate) at different thresholds. The Area Under the ROC Curve (AUC) summarizes the performance of the model across various threshold values.\n",
    "\n",
    "Overall, the confusion matrix is a valuable tool for assessing the performance of classification models, particularly in situations where different types of errors (false positives and false negatives) have varying consequences. It helps data scientists and machine learning practitioners make informed decisions about model selection and optimization based on the specific goals and trade-offs of the problem at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example:\n",
    "\n",
    "We define the actual labels (ground truth) and the predicted labels from a classification model.\n",
    "We create a confusion matrix using sklearn.metrics.confusion_matrix.\n",
    "We print the confusion matrix, which displays True Positives (TP), True Negatives (TN), False Positives (FP), and False Negatives (FN).\n",
    "We calculate accuracy, precision, recall, and F1 score using the values extracted from the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Actual labels (ground truth)\n",
    "actual_labels = [1, 0, 1, 1, 0, 0, 1, 0, 1, 0]\n",
    "\n",
    "# Predicted labels\n",
    "predicted_labels = [1, 0, 1, 1, 1, 0, 0, 1, 1, 0]\n",
    "\n",
    "# Create a confusion matrix\n",
    "confusion = confusion_matrix(actual_labels, predicted_labels)\n",
    "\n",
    "# Define class labels\n",
    "class_names = ['Negative (0)', 'Positive (1)']\n",
    "\n",
    "# Create a heatmap to visualize the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(confusion, interpolation='nearest', cmap=plt.get_cmap('Blues'))\n",
    "plt.title('Confusion Matrix')\n",
    "plt.colorbar()\n",
    "\n",
    "tick_marks = range(len(class_names))\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "\n",
    "for i in range(len(class_names)):\n",
    "    for j in range(len(class_names)):\n",
    "        plt.text(j, i, str(confusion[i, j]), horizontalalignment='center', verticalalignment='center', color='black')\n",
    "\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6. Provide an example of a confusion matrix and explain how precision, recall, and F1 score can be calculated from it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider a binary classification problem where we want to evaluate a model that predicts whether an email is spam or not (ham). We have a confusion matrix like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|                  | Actual Ham (0) | Actual Spam (1) |\n",
    "|------------------|-----------------|------------------|\n",
    "| Predicted Ham    | 800 (TN)        | 50 (FP)          |\n",
    "| Predicted Spam   | 20 (FN)         | 130 (TP)         |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this confusion matrix:\n",
    "\n",
    "- True Negative (TN): The model correctly predicted 800 instances as \"ham\" (negative class).\n",
    "- False Positive (FP): The model incorrectly predicted 50 instances as \"spam\" when they were \"ham.\"\n",
    "- False Negative (FN): The model incorrectly predicted 20 instances as \"ham\" when they were \"spam.\"\n",
    "- True Positive (TP): The model correctly predicted 130 instances as \"spam.\"\n",
    "\n",
    "Now, let's calculate precision, recall, and F1 score:\n",
    "\n",
    "1. Precision (Positive Predictive Value) measures the accuracy of positive predictions. It's calculated as:\n",
    "Precision = TP / (TP + FP) = 130 / (130 + 50) = 130 / 180 ≈ 0.722\n",
    "So, the precision is approximately 0.722, which means 72.2% of the emails predicted as \"spam\" are indeed \"spam.\"\n",
    "\n",
    "2. Recall (Sensitivity or True Positive Rate) measures the ability of the model to identify all relevant instances. It's calculated as:\n",
    "Recall = TP / (TP + FN) = 130 / (130 + 20) = 130 / 150 ≈ 0.867\n",
    "So, the recall is approximately 0.867, which means the model can correctly identify 86.7% of all \"spam\" emails.\n",
    "\n",
    "3. F1 Score is the harmonic mean of precision and recall, providing a balance between these two metrics. It's calculated as:\n",
    "F1 Score = 2 * (Precision * Recall) / (Precision + Recall) = 2 * (0.722 * 0.867) / (0.722 + 0.867) ≈ 0.787\n",
    "The F1 score is approximately 0.787, which indicates a trade-off between precision and recall. It's useful when there's a need to balance false positives and false negatives in the classification problem.\n",
    "\n",
    "In this example, the model has a precision of 0.722, which means it's relatively accurate when it predicts an email as \"spam.\" The recall of 0.867 indicates that it can correctly identify a large portion of actual \"spam\" emails. The F1 score of 0.787 balances these metrics, providing an overall assessment of the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.722\n",
      "Recall: 0.867\n",
      "F1 Score: 0.788\n"
     ]
    }
   ],
   "source": [
    "# Confusion matrix values\n",
    "TN = 800  # True Negative\n",
    "FP = 50   # False Positive\n",
    "FN = 20   # False Negative\n",
    "TP = 130  # True Positive\n",
    "\n",
    "# Calculate precision\n",
    "precision = TP / (TP + FP)\n",
    "print(f'Precision: {precision:.3f}')\n",
    "\n",
    "# Calculate recall\n",
    "recall = TP / (TP + FN)\n",
    "print(f'Recall: {recall:.3f}')\n",
    "\n",
    "# Calculate F1 score\n",
    "f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "print(f'F1 Score: {f1_score:.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q7. Discuss the importance of choosing an appropriate evaluation metric for a classification problem and explain how this can be done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing an appropriate evaluation metric is crucial in machine learning, especially for classification problems, because it helps us understand how well our model is performing and whether it's aligned with the specific goals and requirements of the task. Different metrics highlight different aspects of classification performance, and selecting the right one depends on the nature of the problem and the relative costs of different types of errors.\n",
    "\n",
    "Here are some common classification evaluation metrics and their importance:\n",
    "\n",
    "1. Accuracy:\n",
    "Definition: (TP + TN) / (TP + TN + FP + FN)\n",
    "Importance: It measures the overall correctness of the model's predictions. However, it may not be suitable for imbalanced datasets, where one class significantly outnumbers the other.\n",
    "\n",
    "2. Precision:\n",
    "Definition: TP / (TP + FP)\n",
    "Importance: Precision measures the accuracy of positive predictions. It's crucial when false positives are costly (e.g., in medical diagnoses). High precision indicates that the model rarely misclassifies the positive class.\n",
    "\n",
    "3. Recall (Sensitivity or True Positive Rate):\n",
    "Definition: TP / (TP + FN)\n",
    "Importance: Recall measures the model's ability to identify all relevant instances of the positive class. It's essential when false negatives are costly (e.g., identifying fraud). High recall indicates that the model captures most positive instances.\n",
    "\n",
    "4. F1 Score:\n",
    "Definition: 2 * (Precision * Recall) / (Precision + Recall)\n",
    "Importance: The F1 score balances precision and recall, making it suitable when there's a trade-off between false positives and false negatives. It provides a single metric to assess the model's overall performance.\n",
    "\n",
    "5. Specificity (True Negative Rate):\n",
    "Definition: TN / (TN + FP)\n",
    "Importance: Specificity measures the model's ability to correctly identify the negative class. It's essential when false positives in the negative class are costly (e.g., drug testing). High specificity indicates that the model rarely misclassifies the negative class.\n",
    "\n",
    "6. Area Under the Receiver Operating Characteristic (ROC AUC):\n",
    "Importance: ROC AUC is useful for evaluating the performance of models in binary classification tasks, particularly when the class distribution is imbalanced. It assesses the model's ability to distinguish between positive and negative classes across various thresholds.\n",
    "\n",
    "7. Logarithmic Loss (Log Loss or Cross-Entropy):\n",
    "Importance: Log loss is a useful metric when you need probabilistic predictions. It quantifies the error between predicted probabilities and actual class labels. Lower log loss values indicate better model performance.\n",
    "\n",
    "To choose the appropriate evaluation metric:\n",
    "\n",
    "1. Understand the Problem: Consider the specific objectives, constraints, and implications of false positives and false negatives in the real-world context.\n",
    "\n",
    "2. Analyze the Data: Examine the distribution of classes in the dataset. Imbalanced datasets may require different metrics.\n",
    "\n",
    "3. Set a Baseline: Establish a baseline metric to compare against when evaluating model performance.\n",
    "\n",
    "4. Consider Multiple Metrics: It's often a good practice to look at multiple metrics simultaneously, especially when the costs of different types of errors are unbalanced.\n",
    "\n",
    "5. Domain Expertise: Consult with domain experts to determine which metric aligns best with the problem.\n",
    "\n",
    "6. Adjust as Needed: Be willing to switch metrics if the problem evolves or if the initial choice doesn't align with the project's goals.\n",
    "\n",
    "In summary, the choice of an appropriate evaluation metric should reflect the specific requirements of the classification problem and the costs associated with different types of classification errors. A well-chosen metric helps guide model development and provides insights into its real-world performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q8. Provide an example of a classification problem where precision is the most important metric, and explain why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common example of a classification problem where precision is the most important metric is in medical diagnoses, especially when the consequences of false positives can be severe. Let's consider a medical scenario:\n",
    "\n",
    "**Classification Problem**: Identifying whether a patient has a rare and potentially life-threatening disease (e.g., a certain type of cancer).\n",
    "\n",
    "In this scenario, precision is the most critical metric because:\n",
    "\n",
    "1. **High Stakes**: The consequences of telling a healthy patient they have the disease (false positive) can lead to unnecessary stress, further invasive tests, and potentially harmful treatments. Ensuring that only true cases are diagnosed is of utmost importance.\n",
    "\n",
    "2. **Rare Disease**: The disease is rare, so the majority of patients will not have it (class imbalance). Therefore, a model that frequently predicts negative (no disease) might have high accuracy but could still fail to identify most positive cases. Maximizing precision helps in correctly identifying those rare positive cases.\n",
    "\n",
    "3. **Medical Resources**: Confirming a positive diagnosis may require expensive or invasive tests, and medical resources are limited. Minimizing false positives ensures that resources are allocated efficiently, and only patients who truly need further testing or treatment are identified.\n",
    "\n",
    "In this case, a high-precision model is essential for ensuring that any positive prediction made by the model is highly reliable, minimizing the risk of false alarms and unnecessary medical interventions. While this may result in some missed positive cases (lower recall), it's a trade-off that prioritizes patient safety and well-being.\n",
    "\n",
    "Overall, precision becomes the primary metric because it directly addresses the practical and ethical considerations in the medical field, where false positives can have severe consequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q9. Provide an example of a classification problem where recall is the most important metric and explain why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A typical example of a classification problem where recall is the most important metric is in the context of email spam detection. In email filtering, recall is prioritized because of the following reasons:\n",
    "\n",
    "**Classification Problem**: Identifying whether an incoming email is spam (junk) or not.\n",
    "\n",
    "**Importance of Recall**:\n",
    "\n",
    "1. **Minimizing False Negatives**: False negatives (ham emails incorrectly classified as spam) in email filtering are highly undesirable. When an important email (e.g., a job offer, legal document, or emergency communication) is erroneously flagged as spam and placed in the spam folder, the user might not see it in a timely manner. This can result in missed opportunities, legal consequences, or even life-threatening situations.\n",
    "\n",
    "2. **User Experience**: Prioritizing recall in spam detection aims to ensure a seamless and user-friendly email experience. Users rely on email filters to remove spam, but they don't want legitimate emails to be inadvertently classified as spam. High recall reduces the chances of users missing important communications.\n",
    "\n",
    "3. **Trade-Off with Precision**: While maximizing recall, there may be some compromise on precision. It means that a few non-spam emails may be classified as spam (false positives), leading to occasional items in the inbox that are indeed spam. However, for many users, it is more acceptable to occasionally see a spam email in the inbox than to miss an important email in the spam folder.\n",
    "\n",
    "In the context of email spam detection, a higher recall rate ensures that legitimate emails are less likely to be mistakenly classified as spam, reducing the risk of missing critical information. This prioritization of recall aligns with the goal of providing users with a safe and efficient email experience, while precision may be a secondary concern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference\n",
    "\n",
    "https://www.ibm.com/topics/decision-trees#:~:text=A%20decision%20tree%20is%20a,internal%20nodes%20and%20leaf%20nodes.\n",
    "\n",
    "https://www.youtube.com/watch?v=ZVR2Way4nwQ\n",
    "\n",
    "https://visualstudiomagazine.com/articles/2023/02/21/scikit-decision-tree.aspx#:~:text=A%20decision%20tree%20is%20a,there%20are%20only%20two%20possibilities.\n",
    "\n",
    "https://towardsdatascience.com/understanding-confusion-matrix-a9ad42dcfd62\n",
    "\n",
    "https://towardsdatascience.com/how-to-choose-the-best-evaluation-metric-for-classification-problems-638e845da334"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
